{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef378715",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cudaq' has no attribute 'gradient'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m expectation_value\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Adjoint Differentiation을 이용한 기울기 추출\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# 파라미터 개수가 많아도 Parameter-shift보다 훨씬 빠르게 계산됩니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m gradients = \u001b[43mcudaq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient\u001b[49m(circuit, hamiltonian, params)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent Expectation Value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcost_function(params)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGradients (first 5): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgradients[:\u001b[32m5\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'cudaq' has no attribute 'gradient'"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "import numpy as np\n",
    "\n",
    "# 1. 시뮬레이션 타겟 설정 (State Vector 방식 + GPU 가속)\n",
    "cudaq.set_target(\"nvidia\")\n",
    "\n",
    "# 2. 파라미터화된 양자 커널 정의\n",
    "qubit_count = 20\n",
    "\n",
    "@cudaq.kernel\n",
    "def circuit(params: list[float]):\n",
    "    # 큐비트 할당\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "    \n",
    "    # 1개 레이어: 각 큐비트에 파라미터화된 RY 회전 게이트 적용\n",
    "    for i in range(qubit_count):\n",
    "        cudaq.ry(params[i], qubits[i])\n",
    "    \n",
    "    # 얽힘을 위한 간단한 CNOT 체인 (선택 사항)\n",
    "    for i in range(qubit_count - 1):\n",
    "        cudaq.cx(qubits[i], qubits[i+1])\n",
    "\n",
    "# 3. 관측자(Observable) 정의 \n",
    "# 여기서는 모든 큐비트의 Z축 측정 합 (Total Magnetization)을 최소화하는 것을 목표로 합니다.\n",
    "hamiltonian = cudaq.spin.z(0)\n",
    "for i in range(1, qubit_count):\n",
    "    hamiltonian += cudaq.spin.z(i)\n",
    "\n",
    "# 4. 초기 파라미터 설정 (100개까지 확장 가능하지만, 현재 예제는 20개)\n",
    "params = np.random.uniform(-np.pi, np.pi, qubit_count)\n",
    "\n",
    "# 5. 기댓값 및 기울기 계산\n",
    "# observe().expectation()은 기댓값을, cudaq.gradient()는 Adjoint 방식으로 기울기를 구합니다.\n",
    "def cost_function(current_params):\n",
    "    expectation_value = cudaq.observe(circuit, hamiltonian, current_params).expectation()\n",
    "    return expectation_value\n",
    "\n",
    "# Adjoint Differentiation을 이용한 기울기 추출\n",
    "# 파라미터 개수가 많아도 Parameter-shift보다 훨씬 빠르게 계산됩니다.\n",
    "gradients = cudaq.gradient(circuit, hamiltonian, params)\n",
    "\n",
    "print(f\"Current Expectation Value: {cost_function(params):.6f}\")\n",
    "print(f\"Gradients (first 5): {gradients[:5]}\")\n",
    "\n",
    "# 6. 간단한 경사 하강법(Gradient Descent) 적용 예시\n",
    "learning_rate = 0.1\n",
    "for step in range(10):\n",
    "    grads = cudaq.gradient(circuit, hamiltonian, params)\n",
    "    params -= learning_rate * grads\n",
    "    if step % 2 == 0:\n",
    "        print(f\"Step {step}: Cost = {cost_function(params):.6f}\")\n",
    "\n",
    "print(f\"Final Optimized Cost: {cost_function(params):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a5609e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Cost = -1.740672\n",
      "Step 2: Cost = -1.942469\n",
      "Step 4: Cost = -2.112394\n",
      "Step 6: Cost = -2.343799\n",
      "Step 8: Cost = -2.742951\n",
      "Final Optimized Cost: -3.423657\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "import numpy as np\n",
    "\n",
    "# 1. 타겟 설정\n",
    "cudaq.set_target(\"nvidia\")\n",
    "\n",
    "qubit_count = 20\n",
    "\n",
    "@cudaq.kernel\n",
    "def circuit(params: list[float]):\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "    for i in range(qubit_count):\n",
    "        ry(params[i], qubits[i])\n",
    "    for i in range(qubit_count - 1):\n",
    "        x.ctrl(qubits[i], qubits[i+1])\n",
    "\n",
    "# 2. Hamiltonian 정의\n",
    "hamiltonian = cudaq.spin.z(0)\n",
    "for i in range(1, qubit_count):\n",
    "    hamiltonian += cudaq.spin.z(i)\n",
    "\n",
    "# 미분 엔진 생성\n",
    "gradient_engine = cudaq.gradients.ParameterShift()\n",
    "\n",
    "# 1. 'function' 인자로 들어갈 callable 정의\n",
    "# 파라미터 리스트를 받아 기댓값(float)을 반환해야 함\n",
    "def loss_func(p):\n",
    "    return cudaq.observe(circuit, hamiltonian, p).expectation()\n",
    "\n",
    "# 초기 파라미터 (list 형식을 선호할 수 있으므로 변환)\n",
    "params = np.random.uniform(-np.pi, np.pi, qubit_count).tolist()\n",
    "\n",
    "learning_rate = 0.1\n",
    "for step in range(10):\n",
    "    # 2. funcAtX 계산 (현재 지점의 기댓값)\n",
    "    current_expect = loss_func(params)\n",
    "    \n",
    "    # 3. compute 호출: (parameter_vector, function, funcAtX)\n",
    "    # 에러 메시지의 서명에 맞춰 인자를 전달합니다.\n",
    "    grads = gradient_engine.compute(params, loss_func, current_expect)\n",
    "    \n",
    "    # 파라미터 업데이트\n",
    "    params = (np.array(params) - learning_rate * np.array(grads)).tolist()\n",
    "    \n",
    "    if step % 2 == 0:\n",
    "        print(f\"Step {step}: Cost = {current_expect:.6f}\")\n",
    "\n",
    "print(f\"Final Optimized Cost: {loss_func(params):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76807eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8692185461439579, 2.035350732655954, -1.3899194438619689, -0.07471885770553888, -2.6916414295746596, -1.905307896869438, -1.398451930839478, 0.03265969298056515, 2.2150740091467807, 0.025502851351576705, -1.8084498073384458, -1.3237087904260014, -1.5600663006838422, 1.9983200935342476, 3.0841819913948054, -1.1796351502170752, -2.5674490641724916, 2.8727235520705454, -1.5955277207608647, -0.7570626681985391]\n",
      "Step 0: Cost = -0.128085\n",
      "Step 2: Cost = -0.202303\n",
      "Step 4: Cost = -0.308173\n",
      "Step 6: Cost = -0.449203\n",
      "Step 8: Cost = -0.623854\n",
      "[-2.448108202295145, 1.4965591924027675, -1.5438802520090578, -0.07511868262220203, -2.6949229849813605, -1.8932338841927179, -1.4018523667820582, 0.03265919487987948, 2.2158947283769277, 0.025496336536467497, -1.8080115023016885, -1.3237922565822007, -1.5600901492065116, 1.9983200215988626, 3.08418198510825, -1.179635229106405, -2.5674490799550447, 2.8727235449277058, -1.5955277708467266, -0.7570626759905824]\n",
      "Final Optimized Cost: -0.828255\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "import numpy as np\n",
    "\n",
    "cudaq.set_target(\"nvidia\")\n",
    "qubit_count = 20\n",
    "\n",
    "@cudaq.kernel\n",
    "def circuit(params: list[float]):\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "    for i in range(qubit_count):\n",
    "        ry(params[i], qubits[i])\n",
    "    for i in range(qubit_count - 1):\n",
    "        x.ctrl(qubits[i], qubits[i+1])\n",
    "\n",
    "hamiltonian = cudaq.spin.z(0)\n",
    "for i in range(1, qubit_count):\n",
    "    hamiltonian += cudaq.spin.z(i)\n",
    "\n",
    "# 미분 엔진 생성\n",
    "gradient_engine = cudaq.gradients.ParameterShift()\n",
    "\n",
    "# 1. 'function' 인자로 들어갈 callable 정의\n",
    "# 파라미터 리스트를 받아 기댓값(float)을 반환해야 함\n",
    "def loss_func(p):\n",
    "    return cudaq.observe(circuit, hamiltonian, p).expectation()\n",
    "\n",
    "# 초기 파라미터 (list 형식을 선호할 수 있으므로 변환)\n",
    "params = np.random.uniform(-np.pi, np.pi, qubit_count).tolist()\n",
    "print(params)\n",
    "learning_rate = 0.1\n",
    "for step in range(10):\n",
    "    # 2. funcAtX 계산 (현재 지점의 기댓값)\n",
    "    current_expect = loss_func(params)\n",
    "    \n",
    "    # 3. compute 호출: (parameter_vector, function, funcAtX)\n",
    "    # 에러 메시지의 서명에 맞춰 인자를 전달합니다.\n",
    "    grads = gradient_engine.compute(params, loss_func, current_expect)\n",
    "    \n",
    "    # 파라미터 업데이트\n",
    "    params = (np.array(params) - learning_rate * np.array(grads)).tolist()\n",
    "    \n",
    "    if step % 2 == 0:\n",
    "        print(f\"Step {step}: Cost = {current_expect:.6f}\")\n",
    "print(params)\n",
    "print(f\"Final Optimized Cost: {loss_func(params):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b939fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Params Count: 48\n",
      "Step 0: MSE Cost = 1.076477\n",
      "Step 2: MSE Cost = 1.061244\n",
      "Step 4: MSE Cost = 1.046238\n",
      "Step 6: MSE Cost = 1.031443\n",
      "Step 8: MSE Cost = 1.016859\n",
      "Step 10: MSE Cost = 1.002506\n",
      "Step 12: MSE Cost = 0.988427\n",
      "Step 14: MSE Cost = 0.974686\n",
      "Step 16: MSE Cost = 0.961359\n",
      "Step 18: MSE Cost = 0.948529\n",
      "Final Optimized MSE: 0.936273\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "import numpy as np\n",
    "\n",
    "cudaq.set_target(\"nvidia\")\n",
    "qubit_count = 16\n",
    "layer_count = 3  # 레이어 반복 횟수\n",
    "\n",
    "@cudaq.kernel\n",
    "def circuit(params: list[float]):\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "    \n",
    "    # 레이어 반복 루프\n",
    "    for layer in range(layer_count):    \n",
    "        # 현재 레이어의 파라미터 시작 인덱스 계산\n",
    "        offset = layer * qubit_count\n",
    "        \n",
    "        # 1. Rotation Layer\n",
    "        for i in range(qubit_count):\n",
    "            ry(params[offset + i], qubits[i])\n",
    "            \n",
    "        # 2. Entanglement Layer (CNOT Chain)\n",
    "        for i in range(qubit_count - 1):\n",
    "            x.ctrl(qubits[i], qubits[i+1])\n",
    "\n",
    "# 각 큐비트별 Z 관측자 리스트\n",
    "observables = [cudaq.spin.z(i) for i in range(qubit_count)]\n",
    "target_values = np.full(qubit_count, -1.0)\n",
    "\n",
    "gradient_engine = cudaq.gradients.ParameterShift()\n",
    "\n",
    "def loss_func(p):\n",
    "    current_expectations = []\n",
    "    for obs in observables:\n",
    "        res = cudaq.observe(circuit, obs, p).expectation()\n",
    "        current_expectations.append(res)\n",
    "    \n",
    "    current_expectations = np.array(current_expectations)\n",
    "    mse = np.mean((current_expectations - target_values)**2)\n",
    "    return float(mse)\n",
    "\n",
    "# 초기 파라미터 개수: 20 * 3 = 60개\n",
    "total_param_count = qubit_count * layer_count\n",
    "params = np.random.uniform(-np.pi, np.pi, total_param_count).tolist()\n",
    "\n",
    "learning_rate = 0.2\n",
    "print(f\"Initial Params Count: {len(params)}\")\n",
    "\n",
    "for step in range(20):\n",
    "    current_mse = loss_func(params)\n",
    "    grads = gradient_engine.compute(params, loss_func, current_mse)\n",
    "    \n",
    "    params = (np.array(params) - learning_rate * np.array(grads)).tolist()\n",
    "    \n",
    "    if step % 2 == 0:\n",
    "        print(f\"Step {step}: MSE Cost = {current_mse:.6f}\")\n",
    "\n",
    "print(f\"Final Optimized MSE: {loss_func(params):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f50f02",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f0dab",
   "metadata": {},
   "source": [
    "### Gen만 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eefd6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudaq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "cudaq.set_target(\"nvidia\")\n",
    "\n",
    "# 1. 환경 설정\n",
    "qubit_count = 16\n",
    "layer_count = 3\n",
    "total_param_count = qubit_count * layer_count\n",
    "\n",
    "# 2. CUDA-Q 양자 커널 (Generator)\n",
    "@cudaq.kernel\n",
    "def kernel_g(params: list[float]):\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "    for layer in range(layer_count):\n",
    "        offset = layer * qubit_count\n",
    "        for i in range(qubit_count):\n",
    "            ry(params[offset + i], qubits[i])\n",
    "        for i in range(qubit_count - 1):\n",
    "            x.ctrl(qubits[i], qubits[i+1])\n",
    "\n",
    "# 3. 고전 Discriminator 정의 (PyTorch)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 4. 초기화\n",
    "discriminator = Discriminator(qubit_count)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.01)\n",
    "\n",
    "# Generator 파라미터 (Leaf Tensor로 설정하여 미분 추적)\n",
    "gen_params = torch.tensor(np.random.uniform(-np.pi, np.pi, total_param_count), \n",
    "                          requires_grad=True, dtype=torch.float32)\n",
    "g_optimizer = torch.optim.Adam([gen_params], lr=0.1)\n",
    "\n",
    "# 관측자 설정 (각 큐비트의 Z 기댓값 추출)\n",
    "observables = [cudaq.spin.z(i) for i in range(qubit_count)]\n",
    "gradient_engine = cudaq.gradients.ParameterShift()\n",
    "\n",
    "print(gen_params)\n",
    "# 5. 학습 루프\n",
    "for step in range(50):\n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "    params_list = gen_params.detach().numpy().tolist()\n",
    "    \n",
    "    # 각 큐비트의 기댓값을 저장할 리스트\n",
    "    current_exp_values = [] \n",
    "    expectations = []\n",
    "    \n",
    "    for obs in observables:\n",
    "        # 현재 파라미터에서의 기댓값을 미리 계산 (funcAtX로 사용)\n",
    "        res = cudaq.observe(kernel_g, obs, params_list).expectation()\n",
    "        current_exp_values.append(res)\n",
    "        expectations.append(res)\n",
    "    \n",
    "    q_output = torch.tensor(expectations, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    # Discriminator 평가 및 Loss 계산\n",
    "    prob_fake = discriminator(q_output)\n",
    "    g_loss = -torch.log(prob_fake + 1e-8)\n",
    "    g_loss.backward()\n",
    "    \n",
    "    # Quantum Gradient 수동 주입\n",
    "    with torch.no_grad():\n",
    "        grad_output = q_output.grad.numpy()\n",
    "        # 누적 전 초기화 (중요)\n",
    "        if gen_params.grad is None:\n",
    "            gen_params.grad = torch.zeros_like(gen_params)\n",
    "        else:\n",
    "            gen_params.grad.zero_()\n",
    "\n",
    "        for i, obs in enumerate(observables):\n",
    "            # 수정된 부분: 세 번째 인자로 current_exp_values[i]를 전달\n",
    "            q_grads = gradient_engine.compute(\n",
    "                params_list, \n",
    "                lambda p: cudaq.observe(kernel_g, obs, p).expectation(),\n",
    "                current_exp_values[i]  # funcAtX 인자 추가\n",
    "            )\n",
    "            \n",
    "            # Chain Rule 적용: dL/d_theta = dL/d_exp * d_exp/d_theta\n",
    "            gen_params.grad += grad_output[i] * torch.tensor(q_grads, dtype=torch.float32)\n",
    "\n",
    "    g_optimizer.step()\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step}: G_Loss = {g_loss.item():.6f}, D_Output = {prob_fake.item():.6f}\")\n",
    "\n",
    "print(gen_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e7a51",
   "metadata": {},
   "source": [
    "### Discri 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2b174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | D_Loss: 0.9221 | G_Loss: 0.5462 | D(Real): 0.5198 | D(Fake): 0.5792\n",
      "Step 10 | D_Loss: 0.3541 | G_Loss: 0.7658 | D(Real): 0.9509 | D(Fake): 0.4650\n",
      "Step 20 | D_Loss: 0.5697 | G_Loss: 0.4828 | D(Real): 0.9979 | D(Fake): 0.6171\n",
      "Step 30 | D_Loss: 0.1044 | G_Loss: 2.8654 | D(Real): 0.8850 | D(Fake): 0.0570\n",
      "Step 40 | D_Loss: 0.0749 | G_Loss: 2.1435 | D(Real): 0.9923 | D(Fake): 0.1172\n",
      "Step 50 | D_Loss: 0.7502 | G_Loss: 0.3762 | D(Real): 0.9992 | D(Fake): 0.6865\n",
      "Step 60 | D_Loss: 0.1744 | G_Loss: 3.9831 | D(Real): 0.7288 | D(Fake): 0.0186\n",
      "Step 70 | D_Loss: 0.0419 | G_Loss: 2.6753 | D(Real): 0.9872 | D(Fake): 0.0689\n",
      "Step 80 | D_Loss: 0.2099 | G_Loss: 1.4633 | D(Real): 0.9924 | D(Fake): 0.2315\n",
      "Step 90 | D_Loss: 0.1166 | G_Loss: 2.2292 | D(Real): 0.9087 | D(Fake): 0.1076\n"
     ]
    }
   ],
   "source": [
    "real_data = torch.full((qubit_count,), -1.0, dtype=torch.float32)\n",
    "\n",
    "for step in range(100):\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. Discriminator (D) 학습 단계\n",
    "    # ==========================================\n",
    "    d_optimizer.zero_grad()\n",
    "    \n",
    "    # 진짜 데이터에 대한 판단 (1.0에 가까워야 함)\n",
    "    prob_real = discriminator(real_data)\n",
    "    d_loss_real = -torch.log(prob_real + 1e-8)\n",
    "    \n",
    "    # 가짜 데이터(G의 출력) 생성\n",
    "    params_list = gen_params.detach().numpy().tolist()\n",
    "    expectations = []\n",
    "    for obs in observables:\n",
    "        res = cudaq.observe(kernel_g, obs, params_list).expectation()\n",
    "        expectations.append(res)\n",
    "    q_output_for_d = torch.tensor(expectations, dtype=torch.float32)\n",
    "    \n",
    "    # 가짜 데이터에 대한 판단 (0.0에 가까워야 함)\n",
    "    prob_fake_d = discriminator(q_output_for_d.detach()) # G의 미분은 끊고 D만 학습\n",
    "    d_loss_fake = -torch.log(1.0 - prob_fake_d + 1e-8)\n",
    "    \n",
    "    # D의 전체 손실 및 업데이트\n",
    "    d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. Generator (G) 학습 단계\n",
    "    # ==========================================\n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "    # G가 만든 데이터를 다시 D에게 입력\n",
    "    # (이미 계산한 expectations를 사용하되, 이번엔 grad를 추적함)\n",
    "    q_output_for_g = torch.tensor(expectations, dtype=torch.float32, requires_grad=True)\n",
    "    prob_fake_g = discriminator(q_output_for_g)\n",
    "    \n",
    "    # G의 Loss: D를 속여서 1.0을 얻는 것이 목표\n",
    "    g_loss = -torch.log(prob_fake_g + 1e-8)\n",
    "    g_loss.backward()\n",
    "    \n",
    "    # CUDA-Q Parameter-Shift로 G 미분 주입\n",
    "    with torch.no_grad():\n",
    "        grad_output = q_output_for_g.grad.numpy()\n",
    "        if gen_params.grad is None:\n",
    "            gen_params.grad = torch.zeros_like(gen_params)\n",
    "        else:\n",
    "            gen_params.grad.zero_()\n",
    "\n",
    "        for i, obs in enumerate(observables):\n",
    "            q_grads = gradient_engine.compute(\n",
    "                params_list, \n",
    "                lambda p: cudaq.observe(kernel_g, obs, p).expectation(),\n",
    "                expectations[i]\n",
    "            )\n",
    "            gen_params.grad += grad_output[i] * torch.tensor(q_grads, dtype=torch.float32)\n",
    "\n",
    "    g_optimizer.step()\n",
    "\n",
    "    # 결과 출력\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step} | D_Loss: {d_loss.item():.4f} | G_Loss: {g_loss.item():.4f} | D(Real): {prob_real.item():.4f} | D(Fake): {prob_fake_g.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62efaa1b",
   "metadata": {},
   "source": [
    "### 배치화?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39222f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training with Batch Size: 8\n",
      "Step   0 | D_Loss: 0.6563 | G_Loss: 0.7346 | D(Real): 0.525 | D(Fake): 0.480\n",
      "Step   5 | D_Loss: 0.3812 | G_Loss: 0.7050 | D(Real): 0.939 | D(Fake): 0.494\n",
      "Step  10 | D_Loss: 0.5460 | G_Loss: 0.4830 | D(Real): 0.985 | D(Fake): 0.617\n",
      "Step  15 | D_Loss: 0.4937 | G_Loss: 0.7801 | D(Real): 0.894 | D(Fake): 0.458\n",
      "Step  20 | D_Loss: 0.4752 | G_Loss: 1.5085 | D(Real): 0.504 | D(Fake): 0.221\n",
      "Step  25 | D_Loss: 0.3421 | G_Loss: 0.9028 | D(Real): 0.845 | D(Fake): 0.405\n",
      "Step  30 | D_Loss: 0.4266 | G_Loss: 1.0567 | D(Real): 0.801 | D(Fake): 0.348\n",
      "Step  35 | D_Loss: 0.9466 | G_Loss: 0.4825 | D(Real): 0.454 | D(Fake): 0.617\n",
      "Step  40 | D_Loss: 0.9599 | G_Loss: 0.4081 | D(Real): 0.595 | D(Fake): 0.665\n",
      "Step  45 | D_Loss: 0.6136 | G_Loss: 0.7759 | D(Real): 0.701 | D(Fake): 0.460\n",
      "Step  50 | D_Loss: 0.4568 | G_Loss: 1.0369 | D(Real): 0.807 | D(Fake): 0.355\n",
      "Step  55 | D_Loss: 0.2107 | G_Loss: 1.8027 | D(Real): 0.906 | D(Fake): 0.165\n",
      "Step  60 | D_Loss: 0.0721 | G_Loss: 2.7325 | D(Real): 0.968 | D(Fake): 0.065\n",
      "Step  65 | D_Loss: 0.1037 | G_Loss: 2.0492 | D(Real): 0.987 | D(Fake): 0.129\n",
      "Step  70 | D_Loss: 0.1609 | G_Loss: 1.8065 | D(Real): 0.975 | D(Fake): 0.164\n",
      "Step  75 | D_Loss: 0.1461 | G_Loss: 2.4078 | D(Real): 0.899 | D(Fake): 0.090\n",
      "Step  80 | D_Loss: 0.0736 | G_Loss: 2.7890 | D(Real): 0.963 | D(Fake): 0.061\n",
      "Step  85 | D_Loss: 0.0891 | G_Loss: 2.5502 | D(Real): 0.984 | D(Fake): 0.078\n",
      "Step  90 | D_Loss: 0.2292 | G_Loss: 1.2462 | D(Real): 0.952 | D(Fake): 0.288\n",
      "Step  95 | D_Loss: 0.1168 | G_Loss: 2.2262 | D(Real): 0.952 | D(Fake): 0.108\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 및 데이터 설정\n",
    "batch_size = 8\n",
    "real_data = torch.full((batch_size, qubit_count), -1.0, dtype=torch.float32)\n",
    "\n",
    "print(f\"Starting Training with Batch Size: {batch_size}\")\n",
    "\n",
    "for step in range(100):\n",
    "    # ==========================================\n",
    "    # 1. Discriminator (D) 학습 단계\n",
    "    # ==========================================\n",
    "    d_optimizer.zero_grad()\n",
    "    \n",
    "    # [Real Data] 판별\n",
    "    prob_real = discriminator(real_data)\n",
    "    d_loss_real = -torch.log(prob_real + 1e-8).mean()\n",
    "    \n",
    "    # [Fake Data] 생성 (양자 회로 실행)\n",
    "    params_np = gen_params.detach().numpy()\n",
    "    # 현재 G의 파라미터를 리스트로 변환 (배치 내 모든 샘플이 현재는 동일한 G를 공유)\n",
    "    params_list = params_np.tolist()\n",
    "    \n",
    "    expectations = []\n",
    "    for obs in observables:\n",
    "        # 단일 파라미터 셋에 대한 기댓값 계산\n",
    "        res = cudaq.observe(kernel_g, obs, params_list).expectation()\n",
    "        expectations.append(res)\n",
    "    \n",
    "    # G의 출력을 배치 형태로 복제하여 D의 입력으로 준비 [batch_size, qubit_count]\n",
    "    q_output_vec = torch.tensor(expectations, dtype=torch.float32)\n",
    "    q_output_batch = q_output_vec.repeat(batch_size, 1)\n",
    "    \n",
    "    # [Fake Data] 판별 (D만 학습하기 위해 detach 사용)\n",
    "    prob_fake_d = discriminator(q_output_batch.detach())\n",
    "    d_loss_fake = -torch.log(1.0 - prob_fake_d + 1e-8).mean()\n",
    "    \n",
    "    # D 업데이트\n",
    "    d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. Generator (G) 학습 단계\n",
    "    # ==========================================\n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "    # G의 미분 경로를 만들기 위해 다시 입력 (requires_grad=True)\n",
    "    q_output_for_g = q_output_vec.repeat(batch_size, 1)\n",
    "    q_output_for_g.requires_grad = True\n",
    "    \n",
    "    prob_fake_g = discriminator(q_output_for_g)\n",
    "    \n",
    "    # G Loss: D가 1(Real)이라고 판단하게 만드는 것이 목표\n",
    "    g_loss = -torch.log(prob_fake_g + 1e-8).mean()\n",
    "    g_loss.backward()\n",
    "    \n",
    "    # 양자 그래디언트 수동 주입 (Parameter-Shift Rule)\n",
    "    with torch.no_grad():\n",
    "        # D로부터 전달된 q_output에 대한 기울기 [batch_size, qubit_count]\n",
    "        # 배치 평균 기울기를 구함\n",
    "        grad_from_d = q_output_for_g.grad.mean(dim=0).numpy() \n",
    "        \n",
    "        if gen_params.grad is None:\n",
    "            gen_params.grad = torch.zeros_like(gen_params)\n",
    "        else:\n",
    "            gen_params.grad.zero_()\n",
    "\n",
    "        # 각 관측자(큐비트)별로 파라미터에 대한 기울기 계산\n",
    "        for i, obs in enumerate(observables):\n",
    "            # d(Expectation) / d(Params)\n",
    "            q_grads = gradient_engine.compute(\n",
    "                params_list, \n",
    "                lambda p: cudaq.observe(kernel_g, obs, p).expectation(),\n",
    "                expectations[i]\n",
    "            )\n",
    "            \n",
    "            # Chain Rule: dL/dp = dL/de * de/dp\n",
    "            # 각 큐비트 i에서 온 기울기를 최종 파라미터 grad에 누적\n",
    "            gen_params.grad += grad_from_d[i] * torch.tensor(q_grads, dtype=torch.float32)\n",
    "\n",
    "    g_optimizer.step()\n",
    "\n",
    "    # 결과 출력\n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step:3d} | D_Loss: {d_loss.item():.4f} | G_Loss: {g_loss.item():.4f} | \"\n",
    "              f\"D(Real): {prob_real.mean().item():.3f} | D(Fake): {prob_fake_g.mean().item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15187650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training with PennyLane (Batch Size: 8)\n",
      "Step   0 | D_Loss: 0.6868 | G_Loss: 1.1120 | D(Real): 0.498 | D(Fake): 0.329\n",
      "Step   5 | D_Loss: 0.2067 | G_Loss: 1.3254 | D(Real): 0.959 | D(Fake): 0.266\n",
      "Step  10 | D_Loss: 0.2275 | G_Loss: 1.0950 | D(Real): 0.998 | D(Fake): 0.335\n",
      "Step  15 | D_Loss: 0.1413 | G_Loss: 1.5111 | D(Real): 1.000 | D(Fake): 0.221\n",
      "Step  20 | D_Loss: 0.4006 | G_Loss: 0.6497 | D(Real): 1.000 | D(Fake): 0.522\n",
      "Step  25 | D_Loss: 0.8933 | G_Loss: 0.2590 | D(Real): 1.000 | D(Fake): 0.772\n",
      "Step  30 | D_Loss: 0.5941 | G_Loss: 0.6097 | D(Real): 0.981 | D(Fake): 0.544\n",
      "Step  35 | D_Loss: 0.7129 | G_Loss: 1.4749 | D(Real): 0.323 | D(Fake): 0.229\n",
      "Step  40 | D_Loss: 0.6682 | G_Loss: 0.4087 | D(Real): 0.693 | D(Fake): 0.665\n",
      "Step  45 | D_Loss: 0.8787 | G_Loss: 0.2895 | D(Real): 0.839 | D(Fake): 0.749\n",
      "Step  50 | D_Loss: 0.5832 | G_Loss: 0.8320 | D(Real): 0.630 | D(Fake): 0.435\n",
      "Step  55 | D_Loss: 0.4363 | G_Loss: 1.1118 | D(Real): 0.645 | D(Fake): 0.329\n",
      "Step  60 | D_Loss: 0.3981 | G_Loss: 0.9340 | D(Real): 0.815 | D(Fake): 0.393\n",
      "Step  65 | D_Loss: 0.4276 | G_Loss: 1.1473 | D(Real): 0.700 | D(Fake): 0.317\n",
      "Step  70 | D_Loss: 0.5968 | G_Loss: 0.8072 | D(Real): 0.602 | D(Fake): 0.446\n",
      "Step  75 | D_Loss: 0.5369 | G_Loss: 1.0218 | D(Real): 0.661 | D(Fake): 0.360\n",
      "Step  80 | D_Loss: 0.3856 | G_Loss: 1.1236 | D(Real): 0.762 | D(Fake): 0.325\n",
      "Step  85 | D_Loss: 0.4607 | G_Loss: 0.7919 | D(Real): 0.899 | D(Fake): 0.453\n",
      "Step  90 | D_Loss: 0.4022 | G_Loss: 0.9476 | D(Real): 0.940 | D(Fake): 0.388\n",
      "Step  95 | D_Loss: 0.2954 | G_Loss: 1.4536 | D(Real): 0.915 | D(Fake): 0.234\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "\n",
    "# 1. 환경 설정\n",
    "qubit_count = 18\n",
    "layer_count = 3\n",
    "batch_size = 8\n",
    "device = qml.device(\"lightning.qubit\", wires=qubit_count)\n",
    "\n",
    "# 2. PennyLane QNode 정의 (Broadcasting & Adjoint 적용)\n",
    "@qml.qnode(device, diff_method=\"adjoint\", interface=\"torch\")\n",
    "def quantum_generator(params):\n",
    "    # params shape: (batch_size, layer_count * qubit_count) 또는 (layer_count * qubit_count,)\n",
    "    # PennyLane은 첫 번째 차원이 batch임을 자동으로 인식하여 병렬 처리함\n",
    "    for layer in range(layer_count):\n",
    "        offset = layer * qubit_count\n",
    "        for i in range(qubit_count):\n",
    "            qml.RY(params[..., offset + i], wires=i)\n",
    "        for i in range(qubit_count - 1):\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "    \n",
    "    # 각 큐비트의 Z 기댓값을 리턴 (배치 처리됨)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(qubit_count)]\n",
    "\n",
    "# 3. Discriminator 정의 (동일)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 4. 초기화\n",
    "discriminator = Discriminator(qubit_count)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.01)\n",
    "\n",
    "# Generator 파라미터 (PennyLane-Torch 인터페이스가 미분을 자동 추적함)\n",
    "gen_params = torch.nn.Parameter(\n",
    "    torch.randn(layer_count * qubit_count) * 0.01, requires_grad=True\n",
    ")\n",
    "g_optimizer = torch.optim.Adam([gen_params], lr=0.1)\n",
    "\n",
    "real_data = torch.full((batch_size, qubit_count), -1.0, dtype=torch.float32)\n",
    "\n",
    "print(f\"Starting Training with PennyLane (Batch Size: {batch_size})\")\n",
    "\n",
    "for step in range(100):\n",
    "    # ==========================================\n",
    "    # 1. Discriminator (D) 학습 단계\n",
    "    # ==========================================\n",
    "    d_optimizer.zero_grad()\n",
    "    \n",
    "    # Real Data 판별\n",
    "    prob_real = discriminator(real_data)\n",
    "    d_loss_real = -torch.log(prob_real + 1e-8).mean()\n",
    "    \n",
    "    # Fake Data 생성 (PennyLane QNode 호출)\n",
    "    # gen_params를 (batch_size, n_params)로 확장하여 브로드캐스팅 적용\n",
    "    batched_params = gen_params.unsqueeze(0).expand(batch_size, -1)\n",
    "    # qml.qnode의 리스트 리턴은 (qubit_count, batch_size) 형식이므로 transpose 필요\n",
    "    q_output = torch.stack(quantum_generator(batched_params)).t()\n",
    "    \n",
    "    # Fake Data 판별 (D만 학습)\n",
    "    prob_fake_d = discriminator(q_output.detach())\n",
    "    d_loss_fake = -torch.log(1.0 - prob_fake_d + 1e-8).mean()\n",
    "    \n",
    "    d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. Generator (G) 학습 단계\n",
    "    # ==========================================\n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "    # G는 D가 Real(1)이라고 믿게 만드는 것이 목표\n",
    "    # Adjoint 미분 덕분에 이 지점에서 바로 backward 가능\n",
    "    prob_fake_g = discriminator(q_output)\n",
    "    g_loss = -torch.log(prob_fake_g + 1e-8).mean()\n",
    "    \n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step:3d} | D_Loss: {d_loss.item():.4f} | G_Loss: {g_loss.item():.4f} | \"\n",
    "              f\"D(Real): {prob_real.mean().item():.3f} | D(Fake): {prob_fake_g.mean().item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944acfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training with PennyLane (Batch Size: 8)\n",
      "Step   0 | D_Loss: 0.6599 | G_Loss: 1.0197 | D(Real): 0.573 | D(Fake): 0.361\n",
      "Step   5 | D_Loss: 0.2024 | G_Loss: 1.2912 | D(Real): 0.974 | D(Fake): 0.275\n",
      "Step  10 | D_Loss: 0.2627 | G_Loss: 0.9648 | D(Real): 0.999 | D(Fake): 0.381\n",
      "Step  15 | D_Loss: 0.2888 | G_Loss: 0.8941 | D(Real): 1.000 | D(Fake): 0.409\n",
      "Step  20 | D_Loss: 0.6139 | G_Loss: 0.4151 | D(Real): 1.000 | D(Fake): 0.660\n",
      "Step  25 | D_Loss: 0.9879 | G_Loss: 0.2715 | D(Real): 0.996 | D(Fake): 0.762\n",
      "Step  30 | D_Loss: 0.5239 | G_Loss: 1.4645 | D(Real): 0.514 | D(Fake): 0.231\n",
      "Step  35 | D_Loss: 0.5932 | G_Loss: 0.7393 | D(Real): 0.531 | D(Fake): 0.477\n",
      "Step  40 | D_Loss: 0.7464 | G_Loss: 0.3862 | D(Real): 0.820 | D(Fake): 0.680\n",
      "Step  45 | D_Loss: 0.5445 | G_Loss: 0.9652 | D(Real): 0.625 | D(Fake): 0.381\n",
      "Step  50 | D_Loss: 0.4344 | G_Loss: 0.9955 | D(Real): 0.694 | D(Fake): 0.370\n",
      "Step  55 | D_Loss: 0.5196 | G_Loss: 0.6378 | D(Real): 0.876 | D(Fake): 0.528\n",
      "Step  60 | D_Loss: 0.5911 | G_Loss: 0.6672 | D(Real): 0.838 | D(Fake): 0.513\n",
      "Step  65 | D_Loss: 0.2831 | G_Loss: 1.7088 | D(Real): 0.774 | D(Fake): 0.181\n",
      "Step  70 | D_Loss: 0.3718 | G_Loss: 1.0581 | D(Real): 0.889 | D(Fake): 0.347\n",
      "Step  75 | D_Loss: 0.2915 | G_Loss: 1.5525 | D(Real): 0.812 | D(Fake): 0.212\n",
      "Step  80 | D_Loss: 0.6195 | G_Loss: 0.6095 | D(Real): 0.800 | D(Fake): 0.544\n",
      "Step  85 | D_Loss: 0.8448 | G_Loss: 0.5733 | D(Real): 0.703 | D(Fake): 0.564\n",
      "Step  90 | D_Loss: 0.5391 | G_Loss: 1.2639 | D(Real): 0.604 | D(Fake): 0.283\n",
      "Step  95 | D_Loss: 0.3398 | G_Loss: 1.2246 | D(Real): 0.876 | D(Fake): 0.294\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "\n",
    "# 1. 환경 설정\n",
    "qubit_count = 18\n",
    "layer_count = 3\n",
    "batch_size = 8\n",
    "device = qml.device(\"lightning.gpu\", wires=qubit_count)\n",
    "\n",
    "# 2. PennyLane QNode 정의 (Broadcasting & Adjoint 적용)\n",
    "@qml.qnode(device, diff_method=\"adjoint\", interface=\"torch\")\n",
    "def quantum_generator(params):\n",
    "    # params shape: (batch_size, layer_count * qubit_count) 또는 (layer_count * qubit_count,)\n",
    "    # PennyLane은 첫 번째 차원이 batch임을 자동으로 인식하여 병렬 처리함\n",
    "    for layer in range(layer_count):\n",
    "        offset = layer * qubit_count\n",
    "        for i in range(qubit_count):\n",
    "            qml.RY(params[..., offset + i], wires=i)\n",
    "        for i in range(qubit_count - 1):\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "    1\n",
    "    # 각 큐비트의 Z 기댓값을 리턴 (배치 처리됨)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(qubit_count)]\n",
    "\n",
    "# 3. Discriminator 정의 (동일)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 4. 초기화\n",
    "discriminator = Discriminator(qubit_count)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.01)\n",
    "\n",
    "# Generator 파라미터 (PennyLane-Torch 인터페이스가 미분을 자동 추적함)\n",
    "gen_params = torch.nn.Parameter(\n",
    "    torch.randn(layer_count * qubit_count) * 0.01, requires_grad=True\n",
    ")\n",
    "g_optimizer = torch.optim.Adam([gen_params], lr=0.1)\n",
    "\n",
    "real_data = torch.full((batch_size, qubit_count), -1.0, dtype=torch.float32)\n",
    "\n",
    "print(f\"Starting Training with PennyLane (Batch Size: {batch_size})\")\n",
    "\n",
    "for step in range(100):\n",
    "    # ==========================================\n",
    "    # 1. Discriminator (D) 학습 단계\n",
    "    # ==========================================\n",
    "    d_optimizer.zero_grad()\n",
    "    \n",
    "    # Real Data 판별\n",
    "    prob_real = discriminator(real_data)\n",
    "    d_loss_real = -torch.log(prob_real + 1e-8).mean()\n",
    "    \n",
    "    # Fake Data 생성 (PennyLane QNode 호출)\n",
    "    # gen_params를 (batch_size, n_params)로 확장하여 브로드캐스팅 적용\n",
    "    batched_params = gen_params.unsqueeze(0).expand(batch_size, -1)\n",
    "    # qml.qnode의 리스트 리턴은 (qubit_count, batch_size) 형식이므로 transpose 필요\n",
    "    q_output = torch.stack(quantum_generator(batched_params)).t()\n",
    "    \n",
    "    # Fake Data 판별 (D만 학습)\n",
    "    prob_fake_d = discriminator(q_output.detach())\n",
    "    d_loss_fake = -torch.log(1.0 - prob_fake_d + 1e-8).mean()\n",
    "    \n",
    "    d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. Generator (G) 학습 단계\n",
    "    # ==========================================\n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "    # G는 D가 Real(1)이라고 믿게 만드는 것이 목표\n",
    "    # Adjoint 미분 덕분에 이 지점에서 바로 backward 가능\n",
    "    prob_fake_g = discriminator(q_output)\n",
    "    g_loss = -torch.log(prob_fake_g + 1e-8).mean()\n",
    "    \n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step:3d} | D_Loss: {d_loss.item():.4f} | G_Loss: {g_loss.item():.4f} | \"\n",
    "              f\"D(Real): {prob_real.mean().item():.3f} | D(Fake): {prob_fake_g.mean().item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e7826e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = qml.qnode(device, diff_method = 'adjoint', interface = 'torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d386a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for obs in observables:\n",
    "    # 단일 파라미터 셋에 대한 기댓값 계산\n",
    "    res = cudaq.observe(kernel_g, obs, params_list).expectation()\n",
    "    expectations.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a11c3",
   "metadata": {},
   "source": [
    "### PL vs CUDAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38e676fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA-Q Execution Time (Batch 8): 0.0593s\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "cudaq.set_target(\"nvidia\")\n",
    "\n",
    "qubit_count = 16\n",
    "layer_count = 3\n",
    "batch_size = 8\n",
    "params_np = np.random.uniform(-np.pi, np.pi, (batch_size, qubit_count * layer_count)).astype(np.float32)\n",
    "\n",
    "@cudaq.kernel\n",
    "def circuit(params: list[float]):\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "    for layer in range(layer_count):\n",
    "        offset = layer * qubit_count\n",
    "        for i in range(qubit_count):\n",
    "            ry(params[offset + i], qubits[i])\n",
    "        for i in range(qubit_count - 1):\n",
    "            x.ctrl(qubits[i], qubits[i+1])\n",
    "\n",
    "hamiltonian = cudaq.spin.z(0) # 성능 측정을 위한 단순 관측\n",
    "\n",
    "start_time = time.time()\n",
    "# for i in range(batch_size):\n",
    "#     # 각 배치 샘플에 대해 실행\n",
    "params_np = [params.tolist() for params in params_np]\n",
    "_ = cudaq.observe(circuit, hamiltonian, params_np[i]).expectation()\n",
    "cudaq_duration = time.time() - start_time\n",
    "\n",
    "print(f\"CUDA-Q Execution Time (Batch {batch_size}): {cudaq_duration:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f609a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PennyLane Execution Time (Batch 8): 0.0769s\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import time\n",
    "\n",
    "qubit_count = 16\n",
    "layer_count = 3\n",
    "batch_size = 8\n",
    "params = pnp.random.uniform(-np.pi, np.pi, (batch_size, layer_count * qubit_count), requires_grad=False)\n",
    "\n",
    "dev = qml.device(\"lightning.qubit\", wires=qubit_count)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(inputs):\n",
    "    # inputs: (layer_count * qubit_count,)\n",
    "    for layer in range(layer_count):\n",
    "        offset = layer * qubit_count\n",
    "        for i in range(qubit_count):\n",
    "            qml.RY(inputs[:, offset + i], wires=i)\n",
    "        for i in range(qubit_count - 1):\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# PennyLane의 배치 실행 (vmap 또는 반복 처리 최적화)\n",
    "start_time = time.time()\n",
    "\n",
    "_ = circuit(params)\n",
    "pennylane_duration = time.time() - start_time\n",
    "\n",
    "print(f\"PennyLane Execution Time (Batch {batch_size}): {pennylane_duration:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0fdb417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gradient Calculation Speed Test (N=48) ---\n",
      "CUDA-Q (Parameter-Shift): 0.2596s\n",
      "PennyLane (Adjoint):    0.0486s\n",
      "Speedup Factor:         5.34x\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 설정\n",
    "qubit_count = 16\n",
    "layer_count = 3\n",
    "N = qubit_count * layer_count # 48\n",
    "params_raw = np.random.uniform(-np.pi, np.pi, N).astype(np.float32)\n",
    "\n",
    "# ==========================================\n",
    "# 1. CUDA-Q: Parameter-Shift (2N 오버헤드)\n",
    "# ==========================================\n",
    "cudaq.set_target(\"nvidia\")\n",
    "\n",
    "@cudaq.kernel\n",
    "def kernel_cudaq(params: list[float]):\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "    for layer in range(layer_count):\n",
    "        offset = layer * qubit_count\n",
    "        for i in range(qubit_count):\n",
    "            ry(params[offset + i], qubits[i])\n",
    "        for i in range(qubit_count - 1):\n",
    "            x.ctrl(qubits[i], qubits[i+1])\n",
    "\n",
    "observable = cudaq.spin.z(0)\n",
    "gradient_engine = cudaq.gradients.ParameterShift()\n",
    "\n",
    "start_cudaq = time.time()\n",
    "# 내부적으로 2N번의 회로 실행 발생\n",
    "cuda_grads = gradient_engine.compute(params_raw.tolist(), \n",
    "                                     lambda p: cudaq.observe(kernel_cudaq, observable, p).expectation(),\n",
    "                                     cudaq.observe(kernel_cudaq, observable, params_raw.tolist()).expectation())\n",
    "duration_cudaq = time.time() - start_cudaq\n",
    "\n",
    "# ==========================================\n",
    "# 2. PennyLane: Adjoint Differentiation (O(1) 수준 오버헤드)\n",
    "# ==========================================\n",
    "dev = qml.device(\"lightning.qubit\", wires=qubit_count)\n",
    "params_pnp = pnp.array(params_raw, requires_grad=True)\n",
    "\n",
    "@qml.qnode(dev, diff_method=\"adjoint\") # 핵심: Adjoint 방식 사용\n",
    "def circuit_pennylane(params):\n",
    "    for layer in range(layer_count):\n",
    "        offset = layer * qubit_count\n",
    "        for i in range(qubit_count):\n",
    "            qml.RY(params[offset + i], wires=i)\n",
    "        for i in range(qubit_count - 1):\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "start_pennylane = time.time()\n",
    "# 단 한 번의 역방향 패스로 모든 기울기 계산\n",
    "pennylane_grad = qml.grad(circuit_pennylane)(params_pnp)\n",
    "duration_pennylane = time.time() - start_pennylane\n",
    "\n",
    "# ==========================================\n",
    "# 결과 출력\n",
    "# ==========================================\n",
    "print(f\"--- Gradient Calculation Speed Test (N={N}) ---\")\n",
    "print(f\"CUDA-Q (Parameter-Shift): {duration_cudaq:.4f}s\")\n",
    "print(f\"PennyLane (Adjoint):    {duration_pennylane:.4f}s\")\n",
    "print(f\"Speedup Factor:         {duration_cudaq / duration_pennylane:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd4513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80d52972",
   "metadata": {},
   "source": [
    "## VQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab251e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudaq\n",
    "import numpy as np\n",
    "\n",
    "# 1. 타겟 설정\n",
    "cudaq.set_target(\"nvidia\")\n",
    "\n",
    "qubit_count = 20\n",
    "\n",
    "@cudaq.kernel\n",
    "def circuit(params: list[float]):\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "    for i in range(qubit_count):\n",
    "        ry(params[i], qubits[i])\n",
    "    for i in range(qubit_count - 1):\n",
    "        x.ctrl(qubits[i], qubits[i+1])\n",
    "\n",
    "# 2. Hamiltonian 정의\n",
    "hamiltonian = cudaq.spin.z(0)\n",
    "for i in range(1, qubit_count):\n",
    "    hamiltonian += cudaq.spin.z(i)\n",
    "\n",
    "# 미분 엔진 생성\n",
    "gradient_engine = cudaq.gradients.ParameterShift()\n",
    "\n",
    "# 1. 'function' 인자로 들어갈 callable 정의\n",
    "# 파라미터 리스트를 받아 기댓값(float)을 반환해야 함\n",
    "def loss_func(p):\n",
    "    return cudaq.observe(circuit, hamiltonian, p).expectation()\n",
    "\n",
    "# 초기 파라미터 (list 형식을 선호할 수 있으므로 변환)\n",
    "params = np.random.uniform(-np.pi, np.pi, qubit_count).tolist()\n",
    "\n",
    "learning_rate = 0.1\n",
    "for step in range(10):\n",
    "    # 2. funcAtX 계산 (현재 지점의 기댓값)\n",
    "    current_expect = loss_func(params)\n",
    "    \n",
    "    # 3. compute 호출: (parameter_vector, function, funcAtX)\n",
    "    # 에러 메시지의 서명에 맞춰 인자를 전달합니다.\n",
    "    grads = gradient_engine.compute(params, loss_func, current_expect)\n",
    "    \n",
    "    # 파라미터 업데이트\n",
    "    params = (np.array(params) - learning_rate * np.array(grads)).tolist()\n",
    "    \n",
    "    if step % 2 == 0:\n",
    "        print(f\"Step {step}: Cost = {current_expect:.6f}\")\n",
    "\n",
    "print(f\"Final Optimized Cost: {loss_func(params):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8394e9",
   "metadata": {},
   "source": [
    "## QNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd698ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "\n",
    "qubit_count = 16\n",
    "dev = qml.device(\"lightning.gpu\", wires=qubit_count)\n",
    "\n",
    "# 1. 노이즈 채널 정의 (예: 탈분극 노이즈)\n",
    "@qml.qnode(dev)\n",
    "def noise_circuit(params):\n",
    "    qml.RY(params[0], wires=0)\n",
    "    # 게이트 연산 후 노이즈 채널 추가\n",
    "    qml.DepolarizingChannel(0.01, wires=0) \n",
    "    \n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.BitFlip(0.05, wires=1) # 비트 플립 노이즈\n",
    "    \n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2b26a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Execution Time: 0.1477s\n",
      "GPU Execution Time: 0.0944s\n",
      "CPU Execution Time: 0.2140s\n",
      "GPU Execution Time: 0.1147s\n",
      "CPU Execution Time: 0.2953s\n",
      "GPU Execution Time: 0.1159s\n",
      "CPU Execution Time: 0.1606s\n",
      "GPU Execution Time: 0.1434s\n",
      "CPU Execution Time: 0.4566s\n",
      "GPU Execution Time: 0.2326s\n",
      "CPU Execution Time: 1.9032s\n",
      "GPU Execution Time: 0.4413s\n",
      "CPU Execution Time: 15.1087s\n",
      "GPU Execution Time: 1.2175s\n",
      "Qubit Count | CPU Time (s) | GPU Time (s) | Speedup\n",
      "          10 |       0.1477 |       0.0944 |    1.56x\n",
      "          12 |       0.2140 |       0.1147 |    1.87x\n",
      "          14 |       0.2953 |       0.1159 |    2.55x\n",
      "          16 |       0.1606 |       0.1434 |    1.12x\n",
      "          18 |       0.4566 |       0.2326 |    1.96x\n",
      "          20 |       1.9032 |       0.4413 |    4.31x\n",
      "          22 |      15.1087 |       1.2175 |   12.41x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pennylane as qml\n",
    "\n",
    "qubit_count = 16\n",
    "dev_cpu = qml.device(\"lightning.qubit\", wires=qubit_count)\n",
    "dev_gpu = qml.device(\"lightning.gpu\", wires=qubit_count)\n",
    "\n",
    "c_t = []\n",
    "g_t = []\n",
    "for q in range(10, 23, 2):\n",
    "    dev_cpu = qml.device(\"lightning.qubit\", wires=q)\n",
    "    # 1. 노이즈 채널 정의 (예: 탈분극 노이즈)\n",
    "    @qml.qnode(dev_cpu)\n",
    "    def noise_circuit_cpu(params):\n",
    "        for i in range(q):\n",
    "\n",
    "            qml.RY(params[0], wires=i)\n",
    "        for i in range(q - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        \n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "    s = time.time()\n",
    "    params = [0.5]\n",
    "    for _ in range(50):\n",
    "        noise_circuit_cpu(params)\n",
    "    cpu_time = time.time() - s\n",
    "    c_t.append(cpu_time)\n",
    "    print(f\"CPU Execution Time: {cpu_time:.4f}s\")\n",
    "\n",
    "    dev_gpu = qml.device(\"lightning.gpu\", wires=q)\n",
    "    @qml.qnode(dev_gpu)\n",
    "    def noise_circuit_gpu(params):\n",
    "        for i in range(q):\n",
    "\n",
    "            qml.RY(params[0], wires=i)\n",
    "        for i in range(q - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        \n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "    dev_gpu = qml.device(\"lightning.gpu\", wires=q)\n",
    "    s = time.time()\n",
    "    for _ in range(50):\n",
    "        noise_circuit_gpu(params)\n",
    "        \n",
    "    gpu_time = time.time() - s\n",
    "    g_t.append(gpu_time)\n",
    "    print(f\"GPU Execution Time: {gpu_time:.4f}s\")\n",
    "\n",
    "print(\"Qubit Count | CPU Time (s) | GPU Time (s) | Speedup\")\n",
    "for i in range(len(c_t)):\n",
    "    speedup = c_t[i] / g_t[i]\n",
    "    print(f\"{10 + i*2:12d} | {c_t[i]:12.4f} | {g_t[i]:12.4f} | {speedup:7.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1101f4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(10, 23, 2), c_t, label='CPU Time', marker='o')\n",
    "plt.plot(range(10, 23, 2), g_t, label='GPU Time', marker='o')\n",
    "plt.xlabel('Qubit Count')\n",
    "plt.ylabel('Execution Time (s)')\n",
    "plt.title('CPU vs GPU Execution Time with Noise Channels')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lqgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
